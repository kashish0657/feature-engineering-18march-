{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30fb249-e354-4235-89da-ed4e696a80ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FEATURE ENGINEERING ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7a8b8-3c7b-46ef-8e15-f30f2aa71257",
   "metadata": {},
   "source": [
    "## 1:- What is the filter method in feature selection , and how does it work ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8140b-8917-40b5-97d4-440c62629837",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:-\n",
    "\n",
    "The filter method is a popular technique used in feature selection to identify and select the most relevant\n",
    "features from a given dataset. It is called a \"filter\" method because it evaluates the relevance of features\n",
    "independently of any specific machine learning algorithm.\n",
    "\n",
    "The filter method assesses the characteristics of individual features by measuring statistical properties,\n",
    "such as correlation or information gain, and ranks the features based on their scores. The underlying \n",
    "assumption is that features with higher scores are more likely to be informative and have a stronger\n",
    "relationship with the target variable.\n",
    "\n",
    "Here are the general steps involved in the filter method for feature selection:\n",
    "\n",
    "a : Feature Evaluation: Each feature is evaluated individually based on certain statistical measures or tests.\n",
    "    Common evaluation techniques include correlation coefficient, chi-square test, mutual information, and ANOVA.\n",
    "\n",
    "b : Feature Ranking: After evaluating each feature, a ranking or score is assigned to indicate its relevance or \n",
    "    importance. Features with higher scores are considered more significant.\n",
    "\n",
    "c : Feature Selection: A threshold or a fixed number of top-ranked features is chosen based on domain knowledge,\n",
    "    experimentation, or predefined criteria. The selected features form the subset that will be used for further\n",
    "    analysis or modeling.\n",
    "\n",
    "d : Optional Preprocessing: Once the features are selected, additional preprocessing steps like normalization or\n",
    "    scaling may be performed to ensure compatibility and improve the performance of the subsequent machine learning\n",
    "    algorithm.\n",
    "\n",
    "It's important to note that the filter method considers the features independently of each other and does not\n",
    "take into account their interactions or dependencies. Consequently, it may overlook relevant feature combinations\n",
    "that could be informative together. In such cases, more advanced feature selection techniques like wrapper \n",
    "methods or embedded methods (e.g., recursive feature elimination) may be employed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da525fb-dd7f-437c-9ca4-9ee841c23fba",
   "metadata": {},
   "source": [
    "## 2:- How does the wrapper method differ from the filter method in feature selection ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15fc1ab-f205-451b-b4fd-3021ae4a0c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans :- \n",
    "\n",
    "The filter method:\n",
    "The filter method relies on a predetermined criterion or statistical measure to assess the relevance\n",
    "of each feature independently of the machine learning algorithm. It does not involve training a \n",
    "specific model. Instead, it evaluates the characteristics of individual features, such as their\n",
    "correlation with the target variable, mutual information, chi-square test, or other statistical tests.\n",
    "Features are ranked or assigned scores based on these measures, and a predetermined number of top-ranked\n",
    "features are selected for further analysis. The filter method is computationally less expensive than the\n",
    "wrapper method since it does not involve training models, making it suitable for large datasets.\n",
    "\n",
    "The wrapper method:\n",
    "The wrapper method, on the other hand, evaluates the usefulness of features by using a specific machine\n",
    "learning algorithm as a black box. It involves training and evaluating the model iteratively on different\n",
    "combinations or subsets of features. The performance of the model, typically measured by a chosen evaluation\n",
    "metric (e.g., accuracy, F1 score, or area under the curve), is used to determine the relevance of the features.\n",
    "This method explores the feature space more comprehensively but can be computationally expensive and\n",
    "time-consuming, especially for datasets with a large number of features. Popular wrapper methods include\n",
    "recursive feature elimination (RFE) and forward/backward feature selection.\n",
    "\n",
    "In summary, the key differences between the wrapper method and the filter method are:\n",
    "\n",
    "Approach: The filter method evaluates features independently of the machine learning algorithm, using \n",
    "statistical measures, while the wrapper method uses a machine learning algorithm to assess the relevance\n",
    "of features by training and evaluating models iteratively.\n",
    "\n",
    "Computational Complexity: The filter method is generally less computationally expensive since it doesn't\n",
    "involve model training. In contrast, the wrapper method can be computationally intensive as it requires \n",
    "training and evaluating models on various feature subsets.\n",
    "\n",
    "Exploration of Feature Space: The wrapper method explores the feature space more comprehensively by considering\n",
    "feature combinations, whereas the filter method evaluates features independently.\n",
    "\n",
    "It's worth noting that both methods have their advantages and disadvantages, and the choice between them depends\n",
    "on the specific problem, dataset size, computational resources, and the goals of feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7211134-cb67-45dc-bc30-a379d4d3a1c4",
   "metadata": {},
   "source": [
    "## 3 :- What are some common techniques used in embedded feature seletion method ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a19c4e-46d2-4ae9-98e2-dcf9c2fac2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans :\n",
    "    \n",
    "In the field of machine learning, embedded feature selection methods refer to techniques that perform\n",
    "feature selection as part of the model training process. These methods aim to identify the most relevant\n",
    "features and eliminate or reduce the impact of irrelevant or redundant features, thereby improving the\n",
    "model's performance and interpretability. Here are some common techniques used in embedded feature selection:\n",
    "\n",
    "L1 Regularization (Lasso): L1 regularization adds a penalty term to the model's loss function based on\n",
    "the absolute values of the feature coefficients. This encourages the model to minimize the coefficients\n",
    "of irrelevant features, effectively performing feature selection. Features with zero coefficients are\n",
    "considered irrelevant and can be discarded.\n",
    "\n",
    "L2 Regularization (Ridge): L2 regularization adds a penalty term to the loss function based on the squared\n",
    "magnitudes of the feature coefficients. Although L2 regularization does not directly perform feature selection,\n",
    "it can shrink the coefficients of less important features, reducing their impact on the model.\n",
    "\n",
    "Elastic Net: Elastic Net combines L1 and L2 regularization, providing a balance between feature selection and\n",
    "coefficient shrinkage. It can handle situations where there are highly correlated features by encouraging\n",
    "groups of correlated features to enter or leave the model together.\n",
    "\n",
    "Tree-based methods: Decision tree-based algorithms, such as Random Forest and Gradient Boosting, can naturally\n",
    "perform feature selection as part of their learning process. These methods evaluate the importance of each\n",
    "feature based on how much they contribute to the tree's overall performance. Features with higher importance\n",
    "scores are considered more relevant, while those with low scores can be discarded.\n",
    "\n",
    "Recursive Feature Elimination (RFE): RFE is an iterative technique that starts with a full set of features and\n",
    "progressively removes the least important features based on their coefficients or feature importance scores. It\n",
    "typically uses a model, such as linear regression or SVM, to rank and eliminate features until a desired number\n",
    "or a predefined performance threshold is reached.\n",
    "\n",
    "Regularized Regression: Techniques like Ridge Regression and Lasso Regression can be used as standalone feature\n",
    "selection methods. By adjusting the regularization strength, these methods can control the number of selected\n",
    "features. Ridge Regression tends to keep all features but with reduced coefficients, while Lasso Regression \n",
    "tends to set some coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "Genetic Algorithms: Genetic algorithms apply principles inspired by natural selection to feature selection.\n",
    "They create a population of feature subsets, evaluate their fitness using a specific criterion\n",
    "(e.g., model performance), and then evolve the population by applying genetic operators such as mutation and \n",
    "crossover. Through successive generations, genetic algorithms seek an optimal subset of features.\n",
    "\n",
    "These are just some of the common techniques used in embedded feature selection. The choice of method depends\n",
    "on the specific problem, the type of data, and the underlying model being used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a4e04-f350-4f74-a649-e07dcd9ca8ca",
   "metadata": {},
   "source": [
    "## 4:- What are some drawbacks of using the filter method for feature selection ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f8157-a5b6-493a-9c2d-4171cf8c51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans :\n",
    "\n",
    "The filter method for feature selection is a popular approach that evaluates the relevance of \n",
    "features independently of the chosen machine learning algorithm. While it has certain advantages,\n",
    "such as simplicity and computational efficiency, it also has several drawbacks. Here are some of\n",
    "the limitations and drawbacks associated with the filter method:\n",
    "\n",
    "Independence Assumption: The filter method treats each feature independently and does not consider\n",
    "interactions or dependencies among features. It evaluates features based on individual metrics,\n",
    "such as correlation or statistical significance, without considering their combined effect. This\n",
    "can lead to suboptimal feature subsets when features interact with each other in complex ways.\n",
    "\n",
    "Ignoring Predictive Power: The filter method relies on general statistical measures, such as correlation\n",
    "or mutual information, to assess the relevance of features. These measures do not directly consider the\n",
    "predictive power of features in the context of the specific machine learning task at hand. Consequently,\n",
    "important features that may not have strong correlations or information gain with the target variable can\n",
    "be mistakenly discarded.\n",
    "\n",
    "Inability to Incorporate Feature Interaction: Many machine learning problems involve interactions among \n",
    "features, where the joint effect of multiple features is more informative than individual features alone.\n",
    "The filter method, being a univariate technique, does not capture such feature interactions. Consequently,\n",
    "it may exclude important feature combinations that are relevant for the predictive performance.\n",
    "\n",
    "Sensitivity to Irrelevant Features: The filter method does not account for the presence of irrelevant or\n",
    "redundant features in the dataset. It evaluates each feature individually without considering the impact\n",
    "of other features. As a result, it may include irrelevant features in the selected subset, leading to\n",
    "increased model complexity and potential overfitting.\n",
    "\n",
    "Lack of Adaptability: The filter method selects features based on predefined statistical criteria and does\n",
    "not adapt to the specific characteristics of the dataset or the machine learning algorithm being used. It\n",
    "lacks flexibility in adjusting the feature selection process based on the requirements of the problem,\n",
    "potentially resulting in suboptimal feature subsets for certain tasks.\n",
    "\n",
    "Inability to Handle Feature Dependencies: In scenarios where features are dependent on each other, such\n",
    "as in time series data or text data with n-gram relationships, the filter method may not effectively handle\n",
    "these dependencies. It may not capture the underlying structure or sequential nature of the data, leading \n",
    "to the exclusion of important features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45857894-d18c-4a50-9c6b-b4449d51acc1",
   "metadata": {},
   "source": [
    "## 5:- In which situation would you prefer using the filter method over the wrapper method for feature selecion ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2578ec0-bdf5-406b-a199-bbda406c36bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans :\n",
    "    \n",
    "The filter method and the wrapper method are two common approaches for feature selection in machine\n",
    "learning. Each method has its own advantages and is suitable for different situations. The preference\n",
    "for using one method over the other depends on the specific characteristics of the dataset and the\n",
    "goals of the analysis.\n",
    "\n",
    "Here are some situations where you might prefer using the filter method over the wrapper method for feature selection:\n",
    "\n",
    "Large datasets: If you have a large dataset with a high number of features, the filter method can be\n",
    "computationally more efficient. It typically involves evaluating the relevance of each feature individually,\n",
    "based on statistical measures or correlation with the target variable. This approach can be faster than the\n",
    "wrapper method, which often requires training and evaluating multiple models with different subsets of features.\n",
    "\n",
    "Quick preprocessing step: The filter method is generally faster to implement and provides a quick way to\n",
    "preprocess your data. It allows you to identify potentially informative features before training any models.\n",
    "This can be useful when you want to get a quick overview of the dataset or make an initial assessment of \n",
    "feature importance.\n",
    "\n",
    "Independence of feature subset: The filter method evaluates features independently of each other and their \n",
    "interaction with the learning algorithm. If you have a dataset where the features are largely independent\n",
    "or have weak dependencies, the filter method can be sufficient to identify relevant features. It can capture \n",
    "the individual predictive power of each feature without considering their combined effects.\n",
    "\n",
    "Interpretability: The filter method often relies on statistical measures or simple heuristics to assess\n",
    "feature relevance. This simplicity can lead to more interpretable results, as the selected features are \n",
    "often associated with clear statistical or domain-specific significance. If interpretability is a priority\n",
    "in your analysis, the filter method can be a preferred choice.\n",
    "\n",
    "Exploration and feature engineering: In the early stages of a project, when you are exploring the dataset\n",
    "and performing feature engineering, the filter method can be a useful starting point. By identifying the\n",
    "most relevant features based on statistical measures, you can gain insights into the dataset's structure \n",
    "and potentially guide further feature engineering efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261245a-b6c4-4908-8ee4-57e7f19aac1a",
   "metadata": {},
   "source": [
    "## 6:- In a telecom company , you are working on a project to develop a predictive model for a customer churn . You are unsure of which features to include the model because the dataset contain several different ones . Describe how you would choose the most pertinent attributes for the model using the filter method ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f4c77-64c7-44b0-aded-a7648436bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:\n",
    "    \n",
    "When using the filter method to select the most pertinent attributes for a predictive model, you\n",
    "typically evaluate the relevance of each feature independently of the chosen machine learning algorithm.\n",
    "Here's a step-by-step process you can follow to choose the most relevant attributes for your customer\n",
    "churn predictive model:\n",
    "\n",
    "Understand the Problem: Gain a clear understanding of the problem at hand. Define what constitutes\n",
    "customer churn for your telecom company and identify the factors that are likely to contribute to churn.\n",
    "\n",
    "Data Exploration: Perform exploratory data analysis (EDA) on your dataset to familiarize yourself with the \n",
    "available features. This includes examining the distribution, statistics, and relationships between features.\n",
    "Identify potential issues such as missing values, outliers, or data inconsistencies.\n",
    "\n",
    "Define Evaluation Metric: Determine the evaluation metric you will use to assess the performance of your churn \n",
    "predictive model. Common metrics for binary classification problems like churn prediction include accuracy,\n",
    "precision, recall, F1 score, and area under the receiver operating characteristic curve (AUC-ROC).\n",
    "\n",
    "Identify Relevant Features: Utilize domain knowledge and insights from the EDA to identify features that are\n",
    "likely to be relevant to the churn prediction problem. These features could include customer demographics, \n",
    "usage patterns, service types, billing information, customer support interactions, or any other relevant data\n",
    "points.\n",
    "\n",
    "Statistical Measures: Calculate statistical measures to assess the relevance of individual features. Some\n",
    "commonly used statistical measures for feature selection in the filter method are correlation coefficient,\n",
    "mutual information, chi-squared test, ANOVA, or information gain.\n",
    "\n",
    "Rank Features: Rank the features based on their individual statistical measures. Features with higher \n",
    "statistical measures indicate higher relevance to the churn prediction problem.\n",
    "\n",
    "Set a Threshold: Decide on a threshold for feature selection. You can choose to keep the top N features \n",
    "based on their statistical measures or select features above a certain threshold value.\n",
    "\n",
    "Feature Selection: Select the top-ranked features that meet the threshold criteria as your final set of\n",
    "attributes for the predictive model. Remove irrelevant or redundant features from your dataset.\n",
    "\n",
    "Model Development: Use the selected features to develop your predictive model. Employ suitable machine \n",
    "learning algorithms such as logistic regression, decision trees, random forests, or gradient boosting\n",
    "to train your model on the chosen features.\n",
    "\n",
    "Evaluate and Iterate: Evaluate the performance of your predictive model using the chosen evaluation\n",
    "metric. If the model's performance is not satisfactory, consider refining the feature selection\n",
    "process by adjusting the threshold or exploring different feature engineering techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed92b8-1a71-4505-87d6-007a4a9534af",
   "metadata": {},
   "source": [
    "## 7: You are working on a project to predict the outcomeof a soccer match . You have a large dataset with many features , including player statistics and team ranking . Explain how you would use the Embedded method to select the most relevant features for the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8dd7f3-b1e7-40ec-b278-1f2e769ebc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans :\n",
    "    \n",
    "In the context of feature selection, the embedded method refers to incorporating feature\n",
    "selection within the process of building a predictive model. It combines feature selection\n",
    "with the model training process, allowing the model itself to determine the importance of\n",
    "features while optimizing its performance.\n",
    "\n",
    "To use the embedded method for feature selection in predicting the outcome of a soccer match\n",
    "using a dataset with various features, including player statistics and team ranking, you can\n",
    "follow these steps:\n",
    "\n",
    "Preprocess the dataset: Start by preparing the dataset for analysis. This includes handling\n",
    "missing values, encoding categorical variables, and normalizing or standardizing numerical \n",
    "features as required.\n",
    "\n",
    "Choose a predictive model: Select a suitable machine learning algorithm for predicting soccer\n",
    "match outcomes. Some common choices include logistic regression, decision trees, random forests,\n",
    "or gradient boosting algorithms like XGBoost or LightGBM. The choice of the model may depend on\n",
    "the specifics of the problem and the available dataset.\n",
    "\n",
    "Train the model: Train the selected model using the entire dataset, including all available\n",
    "features. This step allows the model to learn the relationships between the features and the\n",
    "target variable (match outcome).\n",
    "\n",
    "Extract feature importances: After training the model, extract the feature importances or\n",
    "coefficients associated with each feature. Different models have different ways of assigning \n",
    "importances to features. For example, decision trees provide feature importance based on the\n",
    "number of times a feature is used for splitting, while logistic regression provides coefficients\n",
    "representing the feature's contribution to the predicted outcome.\n",
    "\n",
    "Rank the features: Rank the features based on their importance scores or coefficients. Identify\n",
    "the features that contribute most significantly to the model's predictive power. These features \n",
    "are considered more relevant for predicting the outcome of soccer matches.\n",
    "\n",
    "Select the most relevant features: Based on the ranking obtained in the previous step, choose a \n",
    "threshold or a specific number of top-ranked features to keep. You can use techniques like\n",
    "selecting a fixed number of features or selecting the top features until a certain cumulative \n",
    "importance threshold is reached.\n",
    "\n",
    "Retrain the model: Now that you have identified the most relevant features, retrain the model\n",
    "using only these selected features. Removing irrelevant features helps reduce noise and\n",
    "overfitting, improving the model's generalization performance.\n",
    "\n",
    "Evaluate the model: Evaluate the performance of the retrained model using appropriate evaluation\n",
    "metrics such as accuracy, precision, recall, or F1 score. Compare the results with the model \n",
    "trained on the entire feature set to assess the impact of feature selection on the predictive\n",
    "performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c2945-c507-4cfe-a677-ed4ce04c6cbc",
   "metadata": {},
   "source": [
    "## 8: You are working on a project to predict the price of a house based on its features , such as size  , location , and age . You have a limited number of features  , and you want to ensure that you select the most important ones for the model . Explain how you would use the wrapper method to select the best set of feature for the predictor ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ea4cd-1680-44b1-9838-69de13484d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans :\n",
    "    \n",
    "In the wrapper method for feature selection, the goal is to find the optimal subset\n",
    "of features by evaluating different combinations using a specific machine learning\n",
    "algorithm. The wrapper method assesses the performance of the model with different\n",
    "feature subsets and selects the subset that achieves the best performance.\n",
    "\n",
    "Here's a step-by-step explanation of how you can use the wrapper method to select the\n",
    "best set of features for predicting house prices:\n",
    "\n",
    "Choose an evaluation metric: Determine the evaluation metric that you will use to \n",
    "assess the performance of the model. For house price prediction, common metrics include\n",
    "mean squared error (MSE), root mean squared error (RMSE), or mean absolute error (MAE).\n",
    "\n",
    "Split the data: Split your dataset into training and validation sets. The training set\n",
    "will be used for training the model, while the validation set will be used to evaluate \n",
    "the performance of the model with different feature subsets.\n",
    "\n",
    "Define the feature space: Create a list of candidate features based on their relevance to \n",
    "predicting house prices. In your case, this may include features like size, location, and age.\n",
    "\n",
    "Initialize the best feature subset: Start with an empty set as the initial best feature subset.\n",
    "\n",
    "Select feature subsets: Use a search algorithm, such as forward selection or backward\n",
    "elimination, to iteratively build or prune feature subsets. The search algorithm explores\n",
    "different combinations of features and evaluates their performance using the chosen evaluation metric.\n",
    "\n",
    "Forward selection: Start with an empty feature set and iteratively add one feature at a time,\n",
    "evaluating the performance of the model at each step. Add the feature that results in the best\n",
    "improvement in the evaluation metric until no further improvement is observed.\n",
    "\n",
    "Backward elimination: Start with all the features and iteratively remove one feature at a \n",
    "time, evaluating the performance of the model at each step. Remove the feature that results\n",
    "in the least degradation in the evaluation metric until removing any further feature causes\n",
    "a significant drop in performance.\n",
    "\n",
    "Evaluate feature subsets: Train and evaluate the model using the selected feature subsets on\n",
    "the training and validation sets. Use cross-validation techniques, such as k-fold \n",
    "cross-validation, to get a more reliable estimate of the model's performance.\n",
    "\n",
    "Select the best feature subset: Choose the feature subset that yields the best performance\n",
    "on the validation set according to the chosen evaluation metric. This will be your final set\n",
    "of features for predicting house prices.\n",
    "\n",
    "Assess model performance: Once you have the best feature subset, retrain the model using this\n",
    "subset on the entire training dataset. Evaluate the performance of the final model on a \n",
    "separate test dataset to get an unbiased estimate of its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c7315-403d-4e12-ac9b-9b9f52510f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef51a31-eef2-4fc9-a933-a239ecb845a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f979a-7222-4b03-8698-7cc210acea6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a900b-39b7-4f22-aedb-d910d354f508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199bb27-6a8e-4b57-aecd-ee58aac3cd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf218494-5341-40d4-9376-681d31b17c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e26735-524e-4aea-aac1-8299e5b7d5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
